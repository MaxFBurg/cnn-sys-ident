{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for ICLR 2019 paper\n",
    "\n",
    "### A rotation-equivariant convolutional neural network model of primary visual cortex\n",
    "*Alexander S. Ecker, Fabian H. Sinz, Emmanouil Froudarakis, Paul G. Fahey, Santiago A. Cadena, Edgar Y. Walker, Erick Cobos, Jacob Reimer, Andreas S. Tolias, Matthias Bethge*\n",
    "\n",
    "https://openreview.net/forum?id=H1fU8iAqKX\n",
    "\n",
    "This notebook contains the code to build and train all models described in the paper. We start by building and loading a pre-trained model. Then we provide the code to build and train all other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import tensorflow as tf, numpy as np, os, sys\n",
    "p = !pwd\n",
    "p = os.path.dirname(os.path.dirname(p[0]))\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.models import BaseModel, CorePlusReadoutModel\n",
    "from cnn_sys_ident.architectures.cores import StackedRotEquiHermiteConv2dCore\n",
    "from cnn_sys_ident.architectures.readouts import SpatialXFeatureJointL1Readout\n",
    "from cnn_sys_ident.architectures.training import Trainer\n",
    "from data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "NUM_ROTATIONS = 8\n",
    "UPSAMPLING = 2\n",
    "SHARED_BIASES = False\n",
    "FILTER_SIZE = [13, 5, 5]\n",
    "NUM_FILTERS = [16, 16, 16]\n",
    "STRIDE = [1, 1, 1]\n",
    "RATE = [1, 1, 1]\n",
    "PADDING = ['SAME', 'SAME', 'SAME']\n",
    "ACTIVATION_FN = ['soft', 'soft', 'none']\n",
    "REL_SMOOTH_WEIGHT = [1, 0.5, 0.5]\n",
    "REL_SPARSE_WEIGHT = [0, 1, 1]\n",
    "\n",
    "# Readout\n",
    "POSITIVE_FEATURE_WEIGHTS = False\n",
    "INIT_MASKS = 'rand'\n",
    "\n",
    "# Training\n",
    "VAL_STEPS = 50\n",
    "LEARNING_RATE = 0.002\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 5\n",
    "LR_DECAY_STEPS = 2\n",
    "LOG_DIR = 'analysis/iclr2019/checkpoints-repro'\n",
    "LOG_DIR_PRETRAINED = 'analysis/iclr2019/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building and loading a pre-trained model\n",
    "\n",
    "In this section we build a model and load the pre-trained weights from a checkpoint.\n",
    "\n",
    "## Rotation-equivariant model with 16 features used for analyses\n",
    "\n",
    "This is the model with 16 features that we analyze in the paper. The code below just builds and loads the pre-trained model for inference. For training the model from scratch, refer to the code further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseModel(\n",
    "    Dataset.load(),\n",
    "    log_dir=LOG_DIR_PRETRAINED,\n",
    "    log_hash='647bb1d1bd02979996e492b5422eb95f'\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_rotations=NUM_ROTATIONS,\n",
    "    upsampling=UPSAMPLING,\n",
    "    shared_biases=SHARED_BIASES,\n",
    "    filter_size=FILTER_SIZE,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    stride=STRIDE,\n",
    "    rate=RATE,\n",
    "    padding=PADDING,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    ")\n",
    "readout = SpatialXFeatureJointL1Readout(\n",
    "    base,\n",
    "    core.output,\n",
    "    positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "model.load()\n",
    "trainer = Trainer(base, model)   # just for computing the performance\n",
    "trainer.compute_test_corr()      #   (for training, see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building and training models\n",
    "\n",
    "In this section we provide the code for building and training all models and controls shown in the paper. If you just want to load a pre-trained model, follow the pattern above and replace the training code by `model.load()`.\n",
    "\n",
    "## Fig. 2: Model comparison: number of features in last conv layer\n",
    "\n",
    "All models have the same basic architecture: three layers with 16–16–N features (N = 8 ... 48), each at 8 orientations. There are three hyperparameters that we optimize by random search (32 models each): smoothness of convolutional filters (`conv_smooth_weight` $\\in$ [0.001, 0.03]), group sparsity of convolutional filters (`conv_sparse_weight` $\\in$ [0.001, 0.1]) and sparsity of the readout (`readout_sparsity` $\\in$ [0.005, 0.03]). Below we specify the hyperparameters for the best model for each N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [8, 12, 16, 20, 24, 28, 32, 40, 48]\n",
    "conv_smooth_weight = {\n",
    "    8:  0.00781004, 12: 0.00184694, 16: 0.0249692,\n",
    "    20: 0.0257738,  24: 0.00146371, 28: 0.0186784,\n",
    "    32: 0.026082,   40: 0.00232312, 48: 0.00129107}\n",
    "conv_sparse_weight = {\n",
    "    8:  0.0168574,  12: 0.0610123,  16: 0.0152482,\n",
    "    20: 0.0691215,  24: 0.00999698, 28: 0.0187448,\n",
    "    32: 0.0118641,  40: 0.0868334,  48: 0.0644271}\n",
    "readout_sparsity = {\n",
    "    8:  0.0156452,  12: 0.0153464,  16: 0.0170696,\n",
    "    20: 0.0141163,  24: 0.0131784,  28: 0.0124147,\n",
    "    32: 0.0161513,  40: 0.0115895,  48: 0.0163213}\n",
    "log_hash = {   # determines the seed of the random number generator\n",
    "    8:  '8d2912ce0669f4dcc4efa78b970e453c',\n",
    "    12: '4d2e43901a1be496a5e66dc9fec1ed14',\n",
    "    16: '647bb1d1bd02979996e492b5422eb95f',\n",
    "    20: '6babf3b3be2cbd8da50e091966f22e46',\n",
    "    24: '1e34d6f792b506630897ce84fe93a58c',\n",
    "    28: 'a653720bdd962f95b213156f25c80f31',\n",
    "    32: 'd23dd9d3a7149ecc72627115bb940e1e',\n",
    "    40: 'ba65e73469fe90109f22e8204557b646',\n",
    "    48: '37e70606daaa0b2ca13698fee329eec4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_features in N:\n",
    "    base = BaseModel(\n",
    "        Dataset.load(),\n",
    "        log_dir=LOG_DIR,\n",
    "        log_hash=log_hash[num_features]\n",
    "    )\n",
    "    core = StackedRotEquiHermiteConv2dCore(\n",
    "        base,\n",
    "        base.inputs,\n",
    "        num_rotations=NUM_ROTATIONS,\n",
    "        upsampling=UPSAMPLING,\n",
    "        shared_biases=SHARED_BIASES,\n",
    "        filter_size=FILTER_SIZE,\n",
    "        num_filters=[16, 16, num_features],\n",
    "        stride=STRIDE,\n",
    "        rate=RATE,\n",
    "        padding=PADDING,\n",
    "        activation_fn=ACTIVATION_FN,\n",
    "        rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "        rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "        conv_smooth_weight=conv_smooth_weight[num_features],\n",
    "        conv_sparse_weight=conv_sparse_weight[num_features],\n",
    "    )\n",
    "    readout = SpatialXFeatureJointL1Readout(\n",
    "        base,\n",
    "        core.output,\n",
    "        positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    "        init_masks=INIT_MASKS,\n",
    "        readout_sparsity=readout_sparsity[num_features],\n",
    "    )\n",
    "    model = CorePlusReadoutModel(base, core, readout)\n",
    "    trainer = Trainer(base, model)\n",
    "    iter_num, val_loss, test_corr = trainer.fit(\n",
    "        val_steps=VAL_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        patience=PATIENCE,\n",
    "        lr_decay_steps=LR_DECAY_STEPS)\n",
    "    \n",
    "    trainer.compute_test_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1: Performance of our proposed model and various baselines\n",
    "\n",
    "### Rotation-equivariant CNN 3x (16x8)\n",
    "\n",
    "Same as for N=16 above. Repeated here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 16\n",
    "base = BaseModel(\n",
    "    Dataset.load(),\n",
    "    log_dir=LOG_DIR,\n",
    "    log_hash=log_hash[num_features]\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_rotations=NUM_ROTATIONS,\n",
    "    upsampling=UPSAMPLING,\n",
    "    shared_biases=SHARED_BIASES,\n",
    "    filter_size=FILTER_SIZE,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    stride=STRIDE,\n",
    "    rate=RATE,\n",
    "    padding=PADDING,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "    rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "    conv_smooth_weight=conv_smooth_weight[num_features],\n",
    "    conv_sparse_weight=conv_sparse_weight[num_features],\n",
    ")\n",
    "readout = SpatialXFeatureJointL1Readout(\n",
    "    base,\n",
    "    core.output,\n",
    "    positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    "    init_masks=INIT_MASKS,\n",
    "    readout_sparsity=readout_sparsity[num_features],\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "trainer = Trainer(base, model)\n",
    "iter_num, val_loss, test_corr = trainer.fit(\n",
    "    val_steps=VAL_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    lr_decay_steps=LR_DECAY_STEPS)\n",
    "\n",
    "trainer.compute_test_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation-equivariant CNN, but with positive feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseModel(\n",
    "    Dataset.load(),\n",
    "    log_dir=LOG_DIR,\n",
    "    log_hash='a4de905100ac9b78c6a96e8d67f8adfe'\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_rotations=NUM_ROTATIONS,\n",
    "    upsampling=UPSAMPLING,\n",
    "    shared_biases=SHARED_BIASES,\n",
    "    filter_size=FILTER_SIZE,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    stride=STRIDE,\n",
    "    rate=RATE,\n",
    "    padding=PADDING,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "    rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "    conv_smooth_weight=0.00553383,\n",
    "    conv_sparse_weight=0.0715125,\n",
    ")\n",
    "readout = SpatialXFeatureJointL1Readout(\n",
    "    base,\n",
    "    core.output,\n",
    "    positive_feature_weights=True,\n",
    "    init_masks=INIT_MASKS,\n",
    "    readout_sparsity=0.0244531,\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "trainer = Trainer(base, model)\n",
    "iter_num, val_loss, test_corr = trainer.fit(\n",
    "    val_steps=VAL_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    lr_decay_steps=LR_DECAY_STEPS)\n",
    "\n",
    "trainer.compute_test_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation-equivariant CNN, but with non-sparse, L2-regularized feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.readouts import SpatialSparseXFeatureDenseSeparateReadout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseModel(\n",
    "    Dataset.load(),\n",
    "    log_dir=LOG_DIR,\n",
    "    log_hash='9ef7308edab3233c4d02d280ea37bc93'\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_rotations=NUM_ROTATIONS,\n",
    "    upsampling=UPSAMPLING,\n",
    "    shared_biases=SHARED_BIASES,\n",
    "    filter_size=FILTER_SIZE,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    stride=STRIDE,\n",
    "    rate=RATE,\n",
    "    padding=PADDING,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "    rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "    conv_smooth_weight=0.0141237,\n",
    "    conv_sparse_weight=0.00280391,\n",
    ")\n",
    "readout = SpatialSparseXFeatureDenseSeparateReadout(\n",
    "    base,\n",
    "    core.output,\n",
    "    positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    "    init_masks=INIT_MASKS,\n",
    "    mask_sparsity=0.0324413,\n",
    "    feature_l2=0.315181,\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "trainer = Trainer(base, model)\n",
    "iter_num, val_loss, test_corr = trainer.fit(\n",
    "    val_steps=VAL_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    lr_decay_steps=LR_DECAY_STEPS)\n",
    "\n",
    "trainer.compute_test_corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular CNNs with cores of different sizes\n",
    "\n",
    "_Note: When preparing the code for publication, we realized that there is some residual stochasticity in the model fitting procedure (despite fixing all random number generator seeds) that appears to affect these models more than others. You may therefore have to run the model fitting multiple times to reproduce the same performance as reported in the paper. In our experiments, we always ran 32 different initializations, which is why we are confident that the numbers reported are reasonably robust. In the code below, I increased the patience of the early stopping algorithm (from 5 to 10), which leads to more reliable results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.cores import StackedConv2dCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_filter_nums = [\n",
    "    [32, 32, 32],\n",
    "    [64, 64, 64],\n",
    "    [128, 128, 128],\n",
    "    [128, 128, 256],\n",
    "]\n",
    "conv_smooth_weights = [0.0151716, 0.00218237, 0.0277236, 0.0015324]\n",
    "conv_sparse_weights = [0.0219826, 0.0323365, 0.0650177, 0.007974]\n",
    "readout_sparsities = [0.0193531, 0.0261594, 0.0151648, 0.0179]\n",
    "log_hashes = [\n",
    "    '96c4d0cc8869d2b5a4297f13f2cdd422',\n",
    "    'b8c433730fc6d4753f6f910f697b7f4b',\n",
    "    '3bedbbd474249974eb309aeda76ca426',\n",
    "    'f4c477e777c48dac89e61feff11f4327',\n",
    "]\n",
    "for num_filters, conv_smooth_weight, conv_sparse_weight, readout_sparsity, log_hash in zip(\n",
    "        cnn_filter_nums, conv_smooth_weights, conv_sparse_weights, readout_sparsities, log_hashes):\n",
    "    base = BaseModel(\n",
    "        Dataset.load(),\n",
    "        log_dir=LOG_DIR,\n",
    "#         log_dir='checkpoints/aecker_mesonet_data/',\n",
    "        log_hash=log_hash\n",
    "    )\n",
    "    core = StackedConv2dCore(\n",
    "        base,\n",
    "        base.inputs,\n",
    "        filter_size=FILTER_SIZE,\n",
    "        num_filters=num_filters,\n",
    "        stride=STRIDE,\n",
    "        rate=RATE,\n",
    "        padding=PADDING,\n",
    "        activation_fn=ACTIVATION_FN,\n",
    "        rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "        rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "        conv_smooth_weight=conv_smooth_weight,\n",
    "        conv_sparse_weight=conv_sparse_weight,\n",
    "    )\n",
    "    readout = SpatialXFeatureJointL1Readout(\n",
    "        base,\n",
    "        core.output,\n",
    "        positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    "        init_masks=INIT_MASKS,\n",
    "        readout_sparsity=readout_sparsity,\n",
    "    )\n",
    "    model = CorePlusReadoutModel(base, core, readout)\n",
    "    trainer = Trainer(base, model)\n",
    "#     model.base.tf_session.load()\n",
    "    iter_num, val_loss, test_corr = trainer.fit(\n",
    "        val_steps=VAL_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        patience=10,   # in the paper we used 5; more reliable results with 10\n",
    "        lr_decay_steps=LR_DECAY_STEPS)\n",
    "\n",
    "    print(num_filters)\n",
    "    print(trainer.compute_test_corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control: Feature space generalizes to unseen neurons\n",
    "\n",
    "To show that our network learns common features of V1 neurons, we excluded half of the neurons when fitting the network. We then fixed the rotation-equivariant convolutional core and trained only the readout (spatial mask and feature weights) for the other half of the neurons. \n",
    "\n",
    "In terms of implementation, we insert a stop_gradient between the convolutional core and the readout for half of the neurons, which is done in the class for the readout (`SpatialXFeatureJointL1TransferReadout`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.cores import StackedRotEquiHermiteConv2dCore\n",
    "from cnn_sys_ident.architectures.readouts import SpatialXFeatureJointL1TransferReadout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = BaseModel(\n",
    "    Dataset.load(),\n",
    "    log_dir=LOG_DIR,\n",
    "    log_hash='b8f78ead705cb02d09c01f9701067ba2'\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_rotations=NUM_ROTATIONS,\n",
    "    upsampling=UPSAMPLING,\n",
    "    shared_biases=SHARED_BIASES,\n",
    "    filter_size=FILTER_SIZE,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    stride=STRIDE,\n",
    "    rate=RATE,\n",
    "    padding=PADDING,\n",
    "    activation_fn=ACTIVATION_FN,\n",
    "    rel_smooth_weight=REL_SMOOTH_WEIGHT,\n",
    "    rel_sparse_weight=REL_SPARSE_WEIGHT,\n",
    "    conv_smooth_weight=0.0112711,\n",
    "    conv_sparse_weight=0.0492937,\n",
    ")\n",
    "readout = SpatialXFeatureJointL1TransferReadout(\n",
    "    base,\n",
    "    core.output,\n",
    "    k_transfer=2,\n",
    "    positive_feature_weights=POSITIVE_FEATURE_WEIGHTS,\n",
    "    init_masks=INIT_MASKS,\n",
    "    readout_sparsity=0.020616,\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "trainer = Trainer(base, model)\n",
    "iter_num, val_loss, test_corr = trainer.fit(\n",
    "    val_steps=VAL_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    patience=PATIENCE,\n",
    "    lr_decay_steps=LR_DECAY_STEPS)\n",
    "\n",
    "trainer.compute_test_corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
