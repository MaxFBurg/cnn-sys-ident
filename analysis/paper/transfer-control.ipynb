{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import datajoint as dj\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks', rc={'image.cmap': 'bwr'})\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import scipy\n",
    "\n",
    "p = !pwd\n",
    "p = os.path.dirname(os.path.dirname(p[0]))\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting aecker@52.202.15.166:3306\n"
     ]
    }
   ],
   "source": [
    "schema = dj.schema('aecker_mesonet_parameters2', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.mesonet.data import MultiDataset\n",
    "from cnn_sys_ident.mesonet.parameters import Core, Readout, Model, RegPath, Fit\n",
    "from cnn_sys_ident.mesonet import MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = dict(data_hash='cfcd208495d565ef66e7dff9f98764da')\n",
    "model_rel = MODELS['HermiteTransfer'] * MultiDataset() & data_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50 | Loss: 4054.25\n",
      " 100 | Loss: 4043.27\n",
      " 150 | Loss: 4034.93\n",
      " 200 | Loss: 4027.61\n",
      " 250 | Loss: 4025.22\n",
      " 300 | Loss: 4020.33\n",
      " 350 | Loss: 4013.03\n",
      " 400 | Loss: 4008.98\n",
      " 450 | Loss: 4003.91\n",
      " 500 | Loss: 4001.34\n",
      " 550 | Loss: 3997.78\n",
      " 600 | Loss: 3992.25\n",
      " 650 | Loss: 3989.99\n",
      " 700 | Loss: 3988.98\n",
      " 750 | Loss: 3983.52\n",
      " 800 | Loss: 3983.09\n",
      " 850 | Loss: 3982.04\n",
      " 900 | Loss: 3976.21\n",
      " 950 | Loss: 3968.98\n",
      "1000 | Loss: 3969.72\n",
      "1050 | Loss: 3953.77\n",
      "1100 | Loss: 3945.67\n",
      "1150 | Loss: 3938.32\n",
      "1200 | Loss: 3933.66\n",
      "1250 | Loss: 3924.95\n",
      "1300 | Loss: 3937.65\n",
      "1350 | Loss: 3916.31\n",
      "1400 | Loss: 3897.95\n",
      "1450 | Loss: 3882.59\n",
      "1500 | Loss: 3952.35\n",
      "1550 | Loss: 3872.50\n",
      "1600 | Loss: 3847.26\n",
      "1650 | Loss: 3842.64\n",
      "1700 | Loss: 3857.84\n",
      "1750 | Loss: 3843.39\n",
      "1800 | Loss: 3826.85\n",
      "1850 | Loss: 3831.16\n",
      "1900 | Loss: 3810.42\n",
      "1950 | Loss: 3813.26\n",
      "2000 | Loss: 3801.65\n",
      "2050 | Loss: 3892.44\n",
      "2100 | Loss: 3878.66\n",
      "2150 | Loss: 3810.98\n",
      "2200 | Loss: 3816.68\n",
      "2250 | Loss: 3877.05\n",
      "INFO:tensorflow:Restoring parameters from /gpfs01/bethge/home/aecker/lab/projects/microns/cnn-sys-ident/checkpoints/aecker_mesonet_data/66b2e3a4621e1ec8746ccbc675edaf24/model.ckpt\n",
      "Reducing learning rate to 0.000200\n",
      "2050 | Loss: 3785.90\n",
      "2100 | Loss: 3770.91\n",
      "2150 | Loss: 3763.01\n",
      "2200 | Loss: 3756.18\n",
      "2250 | Loss: 3751.95\n",
      "2300 | Loss: 3749.01\n",
      "2350 | Loss: 3755.40\n",
      "2400 | Loss: 3743.52\n",
      "2450 | Loss: 3749.67\n",
      "2500 | Loss: 3738.99\n",
      "2550 | Loss: 3743.82\n",
      "2600 | Loss: 3735.22\n",
      "2650 | Loss: 3738.94\n",
      "2700 | Loss: 3734.05\n",
      "2750 | Loss: 3740.86\n",
      "2800 | Loss: 3734.78\n",
      "2850 | Loss: 3732.83\n",
      "2900 | Loss: 3733.08\n",
      "2950 | Loss: 3728.97\n",
      "3000 | Loss: 3731.74\n",
      "3050 | Loss: 3727.78\n",
      "3100 | Loss: 3727.51\n",
      "3150 | Loss: 3731.16\n",
      "3200 | Loss: 3730.41\n",
      "3250 | Loss: 3728.98\n",
      "3300 | Loss: 3725.94\n",
      "3350 | Loss: 3725.85\n",
      "3400 | Loss: 3734.42\n",
      "3450 | Loss: 3742.07\n",
      "3500 | Loss: 3726.07\n",
      "3550 | Loss: 3741.26\n",
      "3600 | Loss: 3721.85\n",
      "3650 | Loss: 3730.33\n",
      "3700 | Loss: 3723.99\n",
      "3750 | Loss: 3730.45\n",
      "3800 | Loss: 3725.13\n",
      "3850 | Loss: 3721.46\n",
      "3900 | Loss: 3722.00\n",
      "3950 | Loss: 3725.06\n",
      "4000 | Loss: 3720.40\n",
      "4050 | Loss: 3720.15\n",
      "4100 | Loss: 3724.77\n",
      "4150 | Loss: 3721.03\n",
      "4200 | Loss: 3731.28\n",
      "4250 | Loss: 3718.58\n",
      "4300 | Loss: 3725.16\n",
      "4350 | Loss: 3720.80\n",
      "4400 | Loss: 3723.59\n",
      "4450 | Loss: 3726.28\n",
      "4500 | Loss: 3722.55\n",
      "INFO:tensorflow:Restoring parameters from /gpfs01/bethge/home/aecker/lab/projects/microns/cnn-sys-ident/checkpoints/aecker_mesonet_data/66b2e3a4621e1ec8746ccbc675edaf24/model.ckpt\n",
      "Reducing learning rate to 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50 | Loss: 4005.88\n",
      " 100 | Loss: 3938.65\n",
      " 150 | Loss: 3922.54\n",
      " 200 | Loss: 3863.01\n",
      " 250 | Loss: 3873.45\n",
      " 300 | Loss: 3904.43\n",
      " 350 | Loss: 3935.06\n",
      " 250 | Loss: 3834.09\n",
      " 300 | Loss: 3836.99\n",
      " 350 | Loss: 3822.83\n",
      " 400 | Loss: 3817.44\n",
      " 450 | Loss: 3821.42\n",
      " 500 | Loss: 3822.24\n",
      " 550 | Loss: 3811.39\n",
      " 600 | Loss: 3819.93\n",
      " 650 | Loss: 3812.40\n",
      " 700 | Loss: 3803.51\n",
      " 750 | Loss: 3804.02\n",
      " 800 | Loss: 3800.59\n",
      " 850 | Loss: 3804.45\n",
      " 900 | Loss: 3820.41\n",
      "1050 | Loss: 3808.02\n",
      "INFO:tensorflow:Restoring parameters from /gpfs01/bethge/home/aecker/lab/projects/microns/cnn-sys-ident/checkpoints/aecker_mesonet_data/af91cb9e0c3cbf871172e5cccb6c023f/model.ckpt\n",
      "Reducing learning rate to 0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50 | Loss: 4006.07\n",
      " 100 | Loss: 3935.89\n",
      " 150 | Loss: 3968.48\n",
      " 200 | Loss: 3858.40\n",
      " 250 | Loss: 3867.44\n",
      " 300 | Loss: 3848.94\n",
      " 350 | Loss: 3854.48\n",
      " 400 | Loss: 3841.85\n",
      " 450 | Loss: 3846.01\n",
      " 500 | Loss: 3840.82\n",
      " 550 | Loss: 3868.19\n",
      " 600 | Loss: 3886.90\n",
      " 650 | Loss: 3880.95\n",
      " 550 | Loss: 3829.13\n",
      " 600 | Loss: 3834.11\n",
      " 650 | Loss: 3822.51\n",
      " 700 | Loss: 3827.75\n",
      " 750 | Loss: 3825.34\n",
      " 800 | Loss: 3828.43\n",
      " 850 | Loss: 3818.13\n",
      " 900 | Loss: 3816.67\n",
      " 950 | Loss: 3818.13\n",
      "1000 | Loss: 3840.46\n",
      "1050 | Loss: 3831.44\n"
     ]
    }
   ],
   "source": [
    "Fit().populate(model_rel, reserve_jobs=True, suppress_errors=True, order='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.base.data\n",
    "inputs, responses = data.test()\n",
    "feed_dict = {model.base.inputs: inputs, model.base.is_training: False}\n",
    "predictions = model.base.evaluate(model.predictions, feed_dict=feed_dict)\n",
    "rho = [scipy.stats.pearsonr(p, r)[0] for p, r in zip(predictions.T, responses.T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.cores import StackedRotEquiHermiteConv2dCore\n",
    "from cnn_sys_ident.architectures.readouts import SpatialXFeatureJointL1Readout, SpatialSparseXFeatureDenseReadout\n",
    "from cnn_sys_ident.architectures.models import BaseModel, CorePlusReadoutModel\n",
    "from cnn_sys_ident.architectures.training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (MultiDataset() & data_key).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_sys_ident.architectures.utils import soft_threshold, inv_soft_threshold, sta_init\n",
    "\n",
    "class SpatialXFeatureJointL1TransferReadout:\n",
    "    def __init__(self,\n",
    "                 base,\n",
    "                 inputs,\n",
    "                 k_transfer,\n",
    "                 positive_feature_weights=False,\n",
    "                 readout_sparsity=0.017,\n",
    "                 init_masks='sta',\n",
    "                 scope='readout',\n",
    "                 reuse=False,\n",
    "                 **kwargs):\n",
    "        with base.tf_session.graph.as_default():\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                data = base.data\n",
    "                _, num_px_y, num_px_x, num_features = inputs.shape.as_list()\n",
    "                num_neurons = data.num_neurons\n",
    "\n",
    "                # masks\n",
    "                if init_masks == 'sta':\n",
    "                    images_train, responses_train = data.train()\n",
    "                    k = (images_train.shape[1] - num_px_y) // 2\n",
    "                    mask_init = sta_init(images_train, responses_train,\n",
    "                                         max_val=0.01, sd=0.001)[:,k:-k,k:-k]\n",
    "                    mask_init = tf.constant_initializer(mask_init)\n",
    "                else:\n",
    "                    mask_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n",
    "                self.masks = tf.get_variable(\n",
    "                    'masks',\n",
    "                    shape=[num_neurons, num_px_y, num_px_x],\n",
    "                    initializer=mask_init)\n",
    "                self.masks = tf.abs(self.masks, name='positive_masks')\n",
    "\n",
    "                # split masks into neurons used for training the core and neurons\n",
    "                # used for transfer learning (i.e. only readout weights are trained,\n",
    "                # but they do not affect the core). We then insert a stop_gradient\n",
    "                # for the units used only for transfer learning, apply the masks\n",
    "                # and put things back together. This way, there is no gradient flow\n",
    "                # from the units used for transfer learning back into the core.\n",
    "                idx_train = tf.range(num_neurons, delta=k_transfer)\n",
    "                idx_transfer, _ = tf.setdiff1d(tf.range(num_neurons), idx_train)\n",
    "                idx_all = tf.invert_permutation(tf.concat([idx_train, idx_transfer], axis=0))\n",
    "                masks_train = tf.gather(self.masks, idx_train)\n",
    "                masks_transfer = tf.gather(self.masks, idx_transfer)\n",
    "                masked_train = tf.tensordot(masks_train, (inputs),\n",
    "                                            [[1, 2], [1, 2]], name='masked_train')\n",
    "                masked_transfer = tf.tensordot(masks_transfer, tf.stop_gradient(inputs),\n",
    "                                               [[1, 2], [1, 2]], name='masked_transfer')\n",
    "                self.masked = tf.transpose(tf.gather(tf.concat(\n",
    "                            [masked_train, masked_transfer], axis=0), \n",
    "                        idx_all, name='masked'),\n",
    "                    [1, 2, 0])\n",
    "\n",
    "                # feature weights\n",
    "                self.feature_weights = tf.get_variable(\n",
    "                    'feature_weights',\n",
    "                    shape=[num_neurons, num_features],\n",
    "                    initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))\n",
    "                if positive_feature_weights:\n",
    "                    self.feature_weights = tf.abs(self.feature_weights, name='positive_feature_weights')\n",
    "                self.h = tf.reduce_sum(self.masked * tf.transpose(self.feature_weights), 1)\n",
    "\n",
    "                # L1 regularization for readout layer\n",
    "                self.readout_reg = readout_sparsity * tf.reduce_sum(\n",
    "                    tf.reduce_sum(tf.abs(self.masks), [1, 2]) * \\\n",
    "                    tf.reduce_sum(tf.abs(self.feature_weights), 1))\n",
    "                tf.losses.add_loss(self.readout_reg, tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\n",
    "                # bias and output nonlinearity\n",
    "                _, responses = data.train()\n",
    "                bias_init = 0.5 * inv_soft_threshold(responses.mean(axis=0))\n",
    "                self.biases = tf.get_variable(\n",
    "                    'biases',\n",
    "                    shape=[num_neurons],\n",
    "                    initializer=tf.constant_initializer(bias_init))\n",
    "                self.output = tf.identity(soft_threshold(self.h + self.biases), name='output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "base = BaseModel(\n",
    "    data,\n",
    "    log_dir='test-checkpoints',\n",
    "    log_hash='testing_transfer'\n",
    ")\n",
    "core = StackedRotEquiHermiteConv2dCore(\n",
    "    base,\n",
    "    base.inputs,\n",
    "    num_filters=[16, 16, 16],\n",
    "    filter_size = [13, 5, 5],\n",
    "    stride=[1, 1, 1],\n",
    "    rate=[1, 1, 1],\n",
    "    padding=['SAME', 'SAME', 'SAME'],\n",
    "    rel_smooth_weight=[1, 0.5, 0.5],\n",
    "    rel_sparse_weight=[0, 1, 1],\n",
    "    activation_fn=['soft', 'soft', 'none'],\n",
    "    num_rotations=8,\n",
    "    shared_biases=False,\n",
    "    conv_smooth_weight=0.025,\n",
    "    conv_sparse_weight=0.015,\n",
    ")\n",
    "readout = SpatialXFeatureJointL1TransferReadout(\n",
    "    base,\n",
    "    core.output,\n",
    "    k_transfer=2,\n",
    "    positive_feature_weights=False,\n",
    "    init_masks='rand',\n",
    "    readout_sparsity=0.017,\n",
    ")\n",
    "model = CorePlusReadoutModel(base, core, readout)\n",
    "trainer = Trainer(base, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50 | Loss: 4047.85\n",
      " 100 | Loss: 4022.06\n",
      " 150 | Loss: 3993.97\n",
      " 200 | Loss: 3954.39\n",
      " 250 | Loss: 3925.02\n",
      " 300 | Loss: 3901.83\n",
      " 350 | Loss: 3890.11\n",
      " 400 | Loss: 3900.95\n",
      " 450 | Loss: 3867.69\n",
      " 500 | Loss: 3870.01\n",
      " 550 | Loss: 3877.92\n",
      " 600 | Loss: 3842.42\n",
      " 650 | Loss: 3799.27\n",
      " 700 | Loss: 3772.50\n",
      " 750 | Loss: 3783.61\n",
      " 800 | Loss: 3770.68\n",
      " 850 | Loss: 3884.77\n",
      " 900 | Loss: 3761.20\n",
      " 950 | Loss: 3733.74\n",
      "1000 | Loss: 3752.91\n",
      "1050 | Loss: 3731.60\n",
      "1100 | Loss: 3779.30\n",
      "1150 | Loss: 3829.70\n",
      "1200 | Loss: 3732.05\n",
      "1250 | Loss: 3815.78\n",
      "1300 | Loss: 3770.25\n",
      "INFO:tensorflow:Restoring parameters from /gpfs01/bethge/home/aecker/lab/projects/microns/cnn-sys-ident/test-checkpoints/testing_transfer/model.ckpt\n",
      "Reducing learning rate to 0.000200\n",
      "1100 | Loss: 3713.11\n",
      "1150 | Loss: 3697.52\n",
      "1200 | Loss: 3696.02\n",
      "1250 | Loss: 3685.33\n",
      "1300 | Loss: 3682.64\n",
      "1350 | Loss: 3680.14\n",
      "1400 | Loss: 3678.29\n",
      "1450 | Loss: 3680.73\n",
      "1500 | Loss: 3674.82\n",
      "1550 | Loss: 3674.42\n",
      "1600 | Loss: 3672.43\n",
      "1650 | Loss: 3674.66\n",
      "1700 | Loss: 3671.55\n",
      "1750 | Loss: 3672.64\n",
      "1800 | Loss: 3671.51\n",
      "1850 | Loss: 3680.80\n",
      "1900 | Loss: 3668.59\n",
      "1950 | Loss: 3668.14\n",
      "2000 | Loss: 3670.16\n",
      "2050 | Loss: 3674.62\n",
      "2100 | Loss: 3669.61\n",
      "2150 | Loss: 3670.39\n",
      "2200 | Loss: 3673.05\n",
      "INFO:tensorflow:Restoring parameters from /gpfs01/bethge/home/aecker/lab/projects/microns/cnn-sys-ident/test-checkpoints/testing_transfer/model.ckpt\n",
      "Reducing learning rate to 0.000020\n"
     ]
    }
   ],
   "source": [
    "# Using 50% of the neurons for training\n",
    "iter_num, val_loss, test_corr = trainer.fit(val_steps=50, learning_rate=0.002, lr_decay_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
